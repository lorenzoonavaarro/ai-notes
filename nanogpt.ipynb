{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4054c54b-dbb9-4598-bca6-d21e4544d14a",
   "metadata": {},
   "source": [
    "Nanogpt - transformer based character language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69ef98f-f28f-42bc-964b-0977d2fa1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2543e5af-c9fa-4afc-92af-5969f919fff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa39118-48cd-45ab-b6da-f4e15361f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d9b05-9277-436d-8ddd-d68924f7884d",
   "metadata": {},
   "source": [
    "Develop a strategy to tokenize the input text, means **converting the raw text as string to some sequence of integers according to some vocabulary of possible elements**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a9c28-e0da-4fe2-bf7b-0ad9c2412668",
   "metadata": {},
   "source": [
    "# mapping from characters to integers\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for i,s in enumerate(chars)}\n",
    "encode = lambda l: [stoi[c] for c in l] # takes a string, returns a list of integers\n",
    "decode = lambda d: ''.join([itos[c] for c in d]) # takes an array as input, returns a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61de8236-24cb-4aab-9754-e1c75346c7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e5b65-2cc7-45f0-8060-a32917241ec0",
   "metadata": {},
   "source": [
    "Splitting the dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52860be-a2c7-46ba-b206-50847ce5b91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854]) torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2a684-c271-4551-9f58-1ec22e646fff",
   "metadata": {},
   "source": [
    "You do not feed the entire text into the transformer, **not efficient**.\n",
    "\n",
    "Instead you sample random little chunks out of training set, each chunk has a **maximum length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dffac73d-6503-4a71-bd55-d5e9a9f04f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each chunk has many examples feeding into the transformer\n",
    "# you train the examples with contexts between 1 all up to block size\n",
    "# to make the transformer be used to seeting anything in between\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6100ceac-3914-46bf-978f-178087eb885e",
   "metadata": {},
   "source": [
    "Training the transformer with inputs with many contexts from size 1 all up to block size is good for inference, the model gets used to seeing anything in between, while you sampling so it can predict all up to block size, then truncates cause the transformer will never receive more than block size as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "140d7ec7-3160-42d2-ace9-e10a8f51697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target it 47\n",
      "when input is tensor([18, 47]) the target it 56\n",
      "when input is tensor([18, 47, 56]) the target it 57\n",
      "when input is tensor([18, 47, 56, 57]) the target it 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target it 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target it 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target it 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target it 58\n"
     ]
    }
   ],
   "source": [
    "# inputs for the transformer\n",
    "x = train_data[:block_size]\n",
    "# targets for each position in the input, offset by one\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context} the target it {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa30ae-36a6-47ea-b64f-e0e8b7fe45b5",
   "metadata": {},
   "source": [
    "Second dimension: **BATCH DIMENSION**\n",
    "\n",
    "As you sampling the chunks of text, every time you feed it into the transformer, you gonna have many chunks of multiple chunks of text stacked up in a single tensor - efficiency, each one is independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92673a4c-8456-4677-82bf-f87f9cbb3c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target it 43\n",
      "when input is [24, 43] the target it 58\n",
      "when input is [24, 43, 58] the target it 5\n",
      "when input is [24, 43, 58, 5] the target it 57\n",
      "when input is [24, 43, 58, 5, 57] the target it 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target it 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target it 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target it 39\n",
      "when input is [44] the target it 53\n",
      "when input is [44, 53] the target it 56\n",
      "when input is [44, 53, 56] the target it 1\n",
      "when input is [44, 53, 56, 1] the target it 58\n",
      "when input is [44, 53, 56, 1, 58] the target it 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target it 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target it 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target it 1\n",
      "when input is [52] the target it 58\n",
      "when input is [52, 58] the target it 1\n",
      "when input is [52, 58, 1] the target it 58\n",
      "when input is [52, 58, 1, 58] the target it 46\n",
      "when input is [52, 58, 1, 58, 46] the target it 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target it 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target it 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target it 46\n",
      "when input is [25] the target it 17\n",
      "when input is [25, 17] the target it 27\n",
      "when input is [25, 17, 27] the target it 10\n",
      "when input is [25, 17, 27, 10] the target it 0\n",
      "when input is [25, 17, 27, 10, 0] the target it 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target it 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target it 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target it 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y \n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "for b in range(batch_size): # \n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f'when input is {context.tolist()} the target it {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d76b9713-7ad3-465d-9367-769de34b19d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b9031-7e9c-4d71-8b8d-81862bad4381",
   "metadata": {},
   "source": [
    "Feeding into neural networks.\n",
    "\n",
    "Most basic: bigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3697dd6-c56b-48f5-80d5-3d40ffe7b34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(-1)\n",
    "            # loss function \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:] # becomes (B,C)\n",
    "            # apply softmax\n",
    "            probs = F.softmax(logits,dim=-1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to running sequence\n",
    "            idx = torch.cat((idx,idx_next), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df6ffe-08fe-4dbf-8cf0-cc543850b0cb",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4f5eeb6-a233-4589-aae1-5ef28ce90c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19ce8cee-8bf1-4b6a-944b-60feccffdb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.581860065460205\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    # sample batch from the data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36bbc2-ef96-4c5b-8be2-94db7e0eaf9b",
   "metadata": {},
   "source": [
    "### Mathematical trick in self-attention\n",
    "\n",
    "Building first self attention block to process the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ffe5343-0fe4-4cbd-b7bf-2db6a005ce28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a7e86-cb4b-46a7-be26-eaafce5a7560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
